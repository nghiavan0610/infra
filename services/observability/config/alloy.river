// =============================================================================
// Grafana Alloy Configuration - Production
// =============================================================================
// Unified telemetry collector for all infrastructure monitoring
//
// Alloy built-in exporters (no external exporters needed):
//   - Host metrics (CPU, RAM, disk, network)
//   - Container metrics (Docker)
//   - Redis, PostgreSQL, MongoDB, MySQL, Memcached, Kafka, Elasticsearch
//
// Also handles:
//   - OTLP receiver for traces/metrics
//   - Docker log collection
// =============================================================================

// -----------------------------------------------------------------------------
// Logging Configuration
// -----------------------------------------------------------------------------
logging {
  level  = coalesce(sys.env("ALLOY_LOG_LEVEL"), "warn")
  format = "logfmt"
}

// =============================================================================
// METRICS COLLECTION - Built-in Exporters
// =============================================================================

// -----------------------------------------------------------------------------
// Host Metrics (replaces node-exporter)
// -----------------------------------------------------------------------------
prometheus.exporter.unix "host" {
  disable_collectors = ["arp", "bcache", "bonding", "btrfs", "infiniband", "ipvs", "nfs", "nfsd", "tapestats", "xfs", "zfs"]
  filesystem {
    mount_points_exclude = "^/(sys|proc|dev|host|etc|run|snap|var/lib/docker/.+)($$|/)"
    fs_types_exclude     = "^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$$"
  }
}

// -----------------------------------------------------------------------------
// Container Metrics (replaces cadvisor)
// -----------------------------------------------------------------------------
prometheus.exporter.cadvisor "containers" {
  docker_host             = "unix:///var/run/docker.sock"
  docker_only             = true
  storage_duration        = "5m"
  store_container_labels  = true
  // Ensure container names are properly labeled
  allowlist_container_labels = ["com.docker.compose.project", "com.docker.compose.service"]
}

// -----------------------------------------------------------------------------
// Redis Metrics (replaces redis-exporter)
// -----------------------------------------------------------------------------
prometheus.exporter.redis "cache" {
  redis_addr = coalesce(sys.env("REDIS_CACHE_ADDR"), "redis-cache:6379")
  redis_password = sys.env("REDIS_PASSWORD")
}

prometheus.exporter.redis "queue" {
  redis_addr = coalesce(sys.env("REDIS_QUEUE_ADDR"), "redis-queue:6379")
  redis_password = sys.env("REDIS_PASSWORD")
}

// -----------------------------------------------------------------------------
// PostgreSQL Metrics (replaces postgres_exporter)
// -----------------------------------------------------------------------------
prometheus.exporter.postgres "main" {
  data_source_names = [coalesce(sys.env("POSTGRES_DSN"), "postgresql://postgres:postgres@postgres:5432/postgres?sslmode=disable")]
  autodiscovery {
    enabled = true
  }
}

// -----------------------------------------------------------------------------
// Metrics Scraping - Collect from core built-in exporters
// -----------------------------------------------------------------------------
// NOTE: Only include exporters for services that are always running.
// Other services use prometheus.scrape which handles unavailable services gracefully.
prometheus.scrape "integrations" {
  targets = array.concat(
    prometheus.exporter.unix.host.targets,
    prometheus.exporter.cadvisor.containers.targets,
    prometheus.exporter.redis.cache.targets,
    prometheus.exporter.redis.queue.targets,
    prometheus.exporter.postgres.main.targets,
  )
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
}

// =============================================================================
// METRICS COLLECTION - Services with Built-in Metrics
// =============================================================================
// These services expose /metrics endpoints directly, no exporter needed

// -----------------------------------------------------------------------------
// NATS Metrics (built-in monitoring)
// Set NATS_ADDR in .env to enable, leave empty to disable
// -----------------------------------------------------------------------------
prometheus.scrape "nats" {
  targets = [{
    __address__ = sys.env("NATS_ADDR"),
    job = "nats",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// -----------------------------------------------------------------------------
// RabbitMQ Metrics (built-in Prometheus plugin)
// Set RABBITMQ_ADDR in .env to enable, leave empty to disable
// -----------------------------------------------------------------------------
prometheus.scrape "rabbitmq" {
  targets = [{
    __address__ = sys.env("RABBITMQ_ADDR"),
    job = "rabbitmq",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// -----------------------------------------------------------------------------
// Traefik Metrics (built-in)
// Set TRAEFIK_ADDR in .env to enable, leave empty to disable
// -----------------------------------------------------------------------------
prometheus.scrape "traefik" {
  targets = [{
    __address__ = sys.env("TRAEFIK_ADDR"),
    job = "traefik",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// -----------------------------------------------------------------------------
// ClickHouse Metrics (built-in)
// Set CLICKHOUSE_ADDR in .env to enable, leave empty to disable
// -----------------------------------------------------------------------------
prometheus.scrape "clickhouse" {
  targets = [{
    __address__ = sys.env("CLICKHOUSE_ADDR"),
    job = "clickhouse",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// -----------------------------------------------------------------------------
// Meilisearch Metrics (built-in)
// Set MEILISEARCH_ADDR in .env to enable, leave empty to disable
// -----------------------------------------------------------------------------
prometheus.scrape "meilisearch" {
  targets = [{
    __address__ = sys.env("MEILISEARCH_ADDR"),
    job = "meilisearch",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// -----------------------------------------------------------------------------
// Qdrant Metrics (built-in)
// Set QDRANT_ADDR in .env to enable, leave empty to disable
// -----------------------------------------------------------------------------
prometheus.scrape "qdrant" {
  targets = [{
    __address__ = sys.env("QDRANT_ADDR"),
    job = "qdrant",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// -----------------------------------------------------------------------------
// MinIO Metrics (built-in)
// Set MINIO_ADDR in .env to enable, leave empty to disable
// -----------------------------------------------------------------------------
prometheus.scrape "minio" {
  targets = [{
    __address__ = sys.env("MINIO_ADDR"),
    job = "minio",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/minio/v2/metrics/cluster"
}

// =============================================================================
// METRICS COLLECTION - Security & Identity Services
// =============================================================================

// -----------------------------------------------------------------------------
// HashiCorp Vault Metrics
// Set VAULT_ADDR in .env to enable, leave empty to disable
// Requires: telemetry { prometheus_retention_time = "60s" } in vault config
// -----------------------------------------------------------------------------
prometheus.scrape "vault" {
  targets = [{
    __address__ = sys.env("VAULT_ADDR"),
    job = "vault",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/v1/sys/metrics"
  params = { "format" = ["prometheus"] }
}

// -----------------------------------------------------------------------------
// Authentik Metrics (Identity Provider)
// Set AUTHENTIK_ADDR in .env to enable, leave empty to disable
// Metrics exposed on separate port 9300
// -----------------------------------------------------------------------------
prometheus.scrape "authentik" {
  targets = [{
    __address__ = sys.env("AUTHENTIK_ADDR"),
    job = "authentik",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// -----------------------------------------------------------------------------
// CrowdSec Metrics (Security Engine) - DISABLED
// Uncomment if you want to monitor CrowdSec
// Requires: cscli metrics enable
// -----------------------------------------------------------------------------
// prometheus.scrape "crowdsec" {
//   targets = [{
//     __address__ = coalesce(sys.env("CROWDSEC_ADDR"), "crowdsec:6060"),
//     job = "crowdsec",
//   }]
//   forward_to = [prometheus.remote_write.default.receiver]
//   scrape_interval = "30s"
//   metrics_path = "/metrics"
// }

// =============================================================================
// METRICS COLLECTION - Development Tools
// =============================================================================

// -----------------------------------------------------------------------------
// Gitea Metrics (Git Server)
// Set GITEA_ADDR in .env to enable, leave empty to disable
// Requires: [metrics] ENABLED = true in app.ini
// -----------------------------------------------------------------------------
prometheus.scrape "gitea" {
  targets = [{
    __address__ = sys.env("GITEA_ADDR"),
    job = "gitea",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// -----------------------------------------------------------------------------
// n8n Metrics (Workflow Automation)
// Set N8N_ADDR in .env to enable, leave empty to disable
// Requires: N8N_METRICS=true environment variable
// -----------------------------------------------------------------------------
prometheus.scrape "n8n" {
  targets = [{
    __address__ = sys.env("N8N_ADDR"),
    job = "n8n",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// -----------------------------------------------------------------------------
// Self Metrics - Alloy's own metrics
// -----------------------------------------------------------------------------
prometheus.scrape "alloy_self" {
  targets = [{
    __address__ = "localhost:12345",
  }]
  forward_to = [prometheus.remote_write.default.receiver]
  scrape_interval = "60s"
  job_name = "alloy"
}

// -----------------------------------------------------------------------------
// Remote Write to Prometheus
// -----------------------------------------------------------------------------
prometheus.remote_write "default" {
  endpoint {
    url = coalesce(sys.env("PROMETHEUS_URL"), "http://prometheus:9090/api/v1/write")

    queue_config {
      capacity = 10000
      max_shards = 50
      max_samples_per_send = 2000
    }
  }
}

// =============================================================================
// LOG COLLECTION (replaces promtail)
// =============================================================================

// -----------------------------------------------------------------------------
// Docker Log Discovery
// -----------------------------------------------------------------------------
discovery.docker "containers" {
  host = "unix:///var/run/docker.sock"
  refresh_interval = "30s"
}

// -----------------------------------------------------------------------------
// Relabel Docker Logs
// -----------------------------------------------------------------------------
discovery.relabel "docker_logs" {
  targets = discovery.docker.containers.targets

  // Keep container name
  rule {
    source_labels = ["__meta_docker_container_name"]
    regex         = "/(.*)"
    target_label  = "container"
  }

  // Add compose project label if exists
  rule {
    source_labels = ["__meta_docker_container_label_com_docker_compose_project"]
    target_label  = "compose_project"
  }

  // Add compose service label if exists
  rule {
    source_labels = ["__meta_docker_container_label_com_docker_compose_service"]
    target_label  = "compose_service"
  }

  // Extract image name
  rule {
    source_labels = ["__meta_docker_container_image"]
    target_label  = "image"
  }
}

// -----------------------------------------------------------------------------
// Docker Log Source
// -----------------------------------------------------------------------------
loki.source.docker "containers" {
  host = "unix:///var/run/docker.sock"
  targets = discovery.relabel.docker_logs.output
  forward_to = [loki.process.docker.receiver]
  refresh_interval = "30s"
}

// -----------------------------------------------------------------------------
// Log Processing Pipeline
// -----------------------------------------------------------------------------
loki.process "docker" {
  forward_to = [loki.write.default.receiver]

  // Parse JSON logs (many apps output JSON)
  stage.json {
    expressions = {
      level = "level",
      msg   = "msg",
      time  = "time",
    }
    drop_malformed = true
  }

  // Normalize log levels
  stage.label_drop {
    values = ["filename"]
  }

  // Add timestamp if parsed
  stage.timestamp {
    source = "time"
    format = "RFC3339"
  }
}

// -----------------------------------------------------------------------------
// System Logs (disabled - requires /var/log mount)
// -----------------------------------------------------------------------------
// Uncomment below and add /var/log:/var/log:ro volume to enable host log collection
// local.file_match "system_logs" {
//   path_targets = [
//     {__path__ = "/var/log/syslog", job = "syslog"},
//     {__path__ = "/var/log/auth.log", job = "auth"},
//     {__path__ = "/var/log/kern.log", job = "kernel"},
//   ]
// }
//
// loki.source.file "system" {
//   targets    = local.file_match.system_logs.targets
//   forward_to = [loki.write.default.receiver]
// }

// -----------------------------------------------------------------------------
// Loki Write Client
// -----------------------------------------------------------------------------
loki.write "default" {
  endpoint {
    url = coalesce(sys.env("LOKI_URL"), "http://loki:3100/loki/api/v1/push")

    batch_wait = "1s"
    batch_size = "1MiB"
  }
}

// =============================================================================
// TRACES (replaces otel-collector for trace ingestion)
// =============================================================================

// -----------------------------------------------------------------------------
// OTLP Receiver - gRPC & HTTP
// -----------------------------------------------------------------------------
otelcol.receiver.otlp "default" {
  grpc {
    endpoint = "0.0.0.0:4317"
  }
  http {
    endpoint = "0.0.0.0:4318"
  }
  output {
    traces = [otelcol.processor.batch.default.input]
  }
}

// -----------------------------------------------------------------------------
// Batch Processor - Optimize trace sending
// -----------------------------------------------------------------------------
otelcol.processor.batch "default" {
  timeout = "5s"
  send_batch_size = 1000
  output {
    traces = [otelcol.exporter.otlp.tempo.input]
  }
}

// -----------------------------------------------------------------------------
// Export Traces to Tempo
// -----------------------------------------------------------------------------
otelcol.exporter.otlp "tempo" {
  client {
    endpoint = coalesce(sys.env("TEMPO_URL"), "tempo:4317")
    tls {
      insecure = true
    }
  }
}
