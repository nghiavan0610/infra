# =============================================================================
# Observability Stack - Production Configuration (Alloy-based)
# =============================================================================
# Unified telemetry with Grafana Alloy replacing multiple exporters
#
# Components:
#   - Alloy: Unified collector (metrics, logs, traces)
#   - Prometheus: Metrics storage
#   - Loki: Log aggregation
#   - Tempo: Distributed tracing
#   - Grafana: Visualization
#   - Alertmanager: Alert routing
#
# Alloy replaces: node-exporter, cadvisor, redis-exporter, otel-collector, promtail
#
# Usage:
#   docker compose up -d
# =============================================================================

services:
  # ===========================================================================
  # Grafana Alloy - Unified Telemetry Collector
  # ===========================================================================
  # Replaces: node-exporter, cadvisor, redis-exporter, otel-collector, promtail
  alloy:
    image: grafana/alloy:v1.11.0
    container_name: alloy
    restart: unless-stopped
    command:
      - run
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
      - /etc/alloy/config.alloy
    environment:
      - TZ=UTC
      - ALLOY_LOG_LEVEL=${LOG_LEVEL:-warn}
      - PROMETHEUS_URL=http://prometheus:9090/api/v1/write
      - LOKI_URL=http://loki:3100/loki/api/v1/push
      - TEMPO_URL=tempo:4317
      # Core database exporters (Alloy built-in)
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      - REDIS_CACHE_ADDR=${REDIS_CACHE_ADDR:-redis-cache:6379}
      - REDIS_QUEUE_ADDR=${REDIS_QUEUE_ADDR:-redis-queue:6379}
      - POSTGRES_DSN=${POSTGRES_DSN:-}
      # Services with built-in metrics (Alloy scrapes directly)
      - NATS_ADDR=${NATS_ADDR:-}
      - RABBITMQ_ADDR=${RABBITMQ_ADDR:-}
      - TRAEFIK_ADDR=${TRAEFIK_ADDR:-}
      - CLICKHOUSE_ADDR=${CLICKHOUSE_ADDR:-}
      - MEILISEARCH_ADDR=${MEILISEARCH_ADDR:-}
      - QDRANT_ADDR=${QDRANT_ADDR:-}
      - MINIO_ADDR=${MINIO_ADDR:-}
      # Security & Identity services
      - VAULT_ADDR=${VAULT_ADDR:-}
      - AUTHENTIK_ADDR=${AUTHENTIK_ADDR:-}
      - CROWDSEC_ADDR=${CROWDSEC_ADDR:-}
      # Development tools
      - GITEA_ADDR=${GITEA_ADDR:-}
      - N8N_ADDR=${N8N_ADDR:-}
    volumes:
      - ./config/alloy.river:/etc/alloy/config.alloy:ro
      - alloy_data:/var/lib/alloy/data
      # Host metrics access
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      # Docker access (for container metrics & logs)
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      # System logs
      - /var/log:/var/log:ro
    ports:
      - "${OTEL_GRPC_PORT:-4317}:4317"   # OTLP gRPC
      - "${OTEL_HTTP_PORT:-4318}:4318"   # OTLP HTTP
      - "127.0.0.1:12345:12345"          # Alloy UI & metrics
    deploy:
      resources:
        limits:
          cpus: "${ALLOY_CPU_LIMIT:-2}"
          memory: ${ALLOY_MEMORY_LIMIT:-1G}
        reservations:
          cpus: "0.5"
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/12345' 2>/dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    security_opt:
      - no-new-privileges:true
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - observability
      - infra

  # ===========================================================================
  # Prometheus - Metrics Storage
  # ===========================================================================
  prometheus:
    image: prom/prometheus:v2.50.1
    container_name: prometheus
    restart: unless-stopped
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_TIME:-15d}"
      - "--storage.tsdb.retention.size=${PROMETHEUS_RETENTION_SIZE:-10GB}"
      - "--web.enable-lifecycle"
      - "--web.enable-remote-write-receiver"
      - "--enable-feature=native-histograms"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/alerting-rules:/etc/prometheus/alerting-rules:ro
      - ./targets:/etc/prometheus/targets:ro
      - ./secrets:/etc/prometheus/secrets:ro
      - prometheus_data:/prometheus
    ports:
      - "127.0.0.1:${PROMETHEUS_PORT:-9090}:9090"
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: ${PROMETHEUS_MEMORY_LIMIT:-2G}
        reservations:
          cpus: "0.5"
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    security_opt:
      - no-new-privileges:true
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - observability
      - infra
      - traefik-public

  # ===========================================================================
  # Loki - Log Aggregation
  # ===========================================================================
  loki:
    image: grafana/loki:2.9.4
    container_name: loki
    restart: unless-stopped
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./config/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    ports:
      - "127.0.0.1:${LOKI_PORT:-3100}:3100"
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: ${LOKI_MEMORY_LIMIT:-1G}
        reservations:
          cpus: "0.25"
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    security_opt:
      - no-new-privileges:true
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - observability

  # ===========================================================================
  # Tempo - Distributed Tracing
  # ===========================================================================
  tempo:
    image: grafana/tempo:2.4.1
    container_name: tempo
    restart: unless-stopped
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./config/tempo-config.yaml:/etc/tempo.yaml:ro
      - tempo_data:/var/tempo
    ports:
      - "127.0.0.1:${TEMPO_PORT:-3200}:3200"
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: ${TEMPO_MEMORY_LIMIT:-1G}
        reservations:
          cpus: "0.25"
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    security_opt:
      - no-new-privileges:true
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - observability

  # ===========================================================================
  # Alertmanager - Alert Routing & Notifications
  # ===========================================================================
  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: alertmanager
    restart: unless-stopped
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
      - "--web.external-url=${ALERTMANAGER_EXTERNAL_URL:-http://localhost:9093}"
    volumes:
      - ./config/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    ports:
      - "127.0.0.1:${ALERTMANAGER_PORT:-9093}:9093"
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 64M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    security_opt:
      - no-new-privileges:true
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - observability

  # ===========================================================================
  # Grafana - Visualization
  # ===========================================================================
  grafana:
    image: grafana/grafana:10.4.1
    container_name: grafana
    restart: unless-stopped
    environment:
      - TZ=UTC
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-http://localhost:3000}
      - GF_LOG_LEVEL=${LOG_LEVEL:-warn}
      - GF_SECURITY_DISABLE_GRAVATAR=true
      - GF_SECURITY_COOKIE_SECURE=${GRAFANA_COOKIE_SECURE:-false}
      - GF_SECURITY_COOKIE_SAMESITE=strict
      - GF_DATABASE_WAL=true
      - GF_UNIFIED_ALERTING_ENABLED=false
      - GF_ALERTING_ENABLED=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml:ro
      - ./config/grafana-dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml:ro
      - ./dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: ${GRAFANA_MEMORY_LIMIT:-512M}
        reservations:
          cpus: "0.25"
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    security_opt:
      - no-new-privileges:true
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - observability
      - traefik-public
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
      tempo:
        condition: service_healthy

# =============================================================================
# Volumes
# =============================================================================
volumes:
  alloy_data:
    name: observability_alloy_data
  prometheus_data:
    name: observability_prometheus_data
  loki_data:
    name: observability_loki_data
  tempo_data:
    name: observability_tempo_data
  grafana_data:
    name: observability_grafana_data
  alertmanager_data:
    name: observability_alertmanager_data

# =============================================================================
# Networks
# =============================================================================
networks:
  observability:
    driver: bridge
    name: observability
  infra:
    external: true
    name: infra
  traefik-public:
    external: true
    name: traefik-public
